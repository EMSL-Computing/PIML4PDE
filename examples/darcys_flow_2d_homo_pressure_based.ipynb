{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "120ce4dd-a821-41d1-8322-d8d203f88419",
   "metadata": {},
   "source": [
    "# Problem Statement: \n",
    "We aim to solve 2D steady-state flow problems in homogeneous porous media using the Physics-Informed Neural Networks (PINN) approach of PDE solutions and the JAX library for pressure  over a rectangular domain.\n",
    "\n",
    "\n",
    "\n",
    "### **Domain**:\n",
    "$$ \n",
    "\\Omega = \\{ (x, y) \\mid 0 \\leq x \\leq L_1, \\, 0 \\leq y \\leq L_2 \\}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Governing Equation**:\n",
    "$$ \n",
    "\\frac{\\partial^2 P}{\\partial x^2} + \\frac{\\partial^2 P}{\\partial y^2} = 0, \\quad \\text{in } \\Omega.\n",
    "$$ \n",
    "\n",
    "\n",
    "\n",
    "### **Boundary Conditions**\n",
    "\n",
    "**Dirichlet Boundary Conditions** (Prescribed pressure on the left and right boundaries:  \n",
    "   $$ \n",
    "   P(0, y) = P_1, \\quad \\forall y \\in [0, L_2].\n",
    "   $$\n",
    "\n",
    "   $$ \n",
    "   P(L_1, y) = P_2, \\quad \\forall y \\in [0, L_2].\n",
    "   $$\n",
    "\n",
    "**Neumann Boundary Conditions** (No flow on the top and bottom boundaries):  \n",
    "   $$ \n",
    "   \\frac{\\partial P}{\\partial y}(x, 0) = 0, \\quad \\forall x \\in [0, L_1].\n",
    "   $$\n",
    "  \n",
    "   $$ \n",
    "   \\frac{\\partial P}{\\partial y}(x, L_2) = 0, \\quad \\forall x \\in [0, L_1].\n",
    "   $$\n",
    "\n",
    "We solve this problem using Physics-Informed Neural Networks (PINN) and compare it with the Analytical solution for this configuration.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Developed by**:\n",
    "Lal Mamud, Postdoc - Subsurface Modeler, Environmental Subsurface Science Group, Energy & Environment Division, Pacific Northwest National Laboratory, Richland, WA, USA.\n",
    "\n",
    "**Mentors**:\n",
    "Maruti K. Mudunuru and Satish Karra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5377f5-ee15-4af9-b2b8-b4c8cf9c87fc",
   "metadata": {},
   "source": [
    "## 1. Import required Python libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7cff2d-f2ab-41a5-a5cf-402c6e32c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, grad, vmap, jacfwd, jacrev\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac17eb-240a-4fc6-9bd6-72c651691210",
   "metadata": {},
   "source": [
    "## 2. Define Domain and Boundary Conditions, and Generate Collocation Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd0029a-3f2e-4b48-b0e0-11704e54c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = 0.0, 1.0\n",
    "ymin, ymax = 0.0 , 1.0\n",
    "\n",
    "# Define the boundary conditions\n",
    "P1 = 1.0             # at x = 0 for all y_b1, Dirichlet boundary at the left (kPa)  \n",
    "P2 = 0.9             # at x = 1 for all y, Dirichlet boundary at the right (kPa)\n",
    "dP_dy1 = 0           # at y = 0 for all x, Neumann boundary at the bottom (kPa)\n",
    "dP_dy2 = 0           # at y = 1 for all x, Neumann boundary at the top (kPa)\n",
    "\n",
    "# define the number of points on each boundary and \n",
    "N_b = 100          # Number of boundary condition points on each side.\n",
    "N_r = 5_000        # Number of collocation points within the domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2616b63-b3e7-49ec-9d98-290bb0d5c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_BCs_and_colloc(N_b, N_r, ymin, ymax, xmin, xmax, P1, P2, dP_dy1, dP_dy2):\n",
    "    \"\"\"\n",
    "    Generates boundary conditions and collocation points.\n",
    "\n",
    "    Parameters:\n",
    "    - N_b: Number of boundary condition points.\n",
    "    - N_r: Number of collocation points.\n",
    "    - ymin, ymax: Minimum and maximum values for y.\n",
    "    - xmin, xmax: Minimum and maximum values for x.\n",
    "    - left_bc: Boundary condition value at x=0.\n",
    "    - right_bc: Boundary condition value at x=1.\n",
    "\n",
    "    Returns:\n",
    "    - conds: A list of arrays for each boundary condition.\n",
    "    - colloc: An array of collocation points.\n",
    "    \"\"\"\n",
    "    # Generate random number generator keys\n",
    "    bk1, bk2, bk3, bk4, r1k, r2k = jax.random.split(jax.random.PRNGKey(0), 6)\n",
    "\n",
    "    # Left BC: P[0,y] = left_bc\n",
    "    y_b1 = jax.random.uniform(bk1, minval=ymin, maxval=ymax, shape=(N_b, 1))\n",
    "    x_b1 = jnp.zeros_like(y_b1)\n",
    "    bc_1 = jnp.ones_like(y_b1) * P1\n",
    "    BC_1 = jnp.concatenate([x_b1, y_b1, bc_1], axis=1)\n",
    "\n",
    "    # Right BC: P[1,y] = right_bc\n",
    "    y_b2 = jax.random.uniform(bk2, minval=ymin, maxval=ymax, shape=(N_b, 1))\n",
    "    x_b2 = jnp.ones_like(y_b2)\n",
    "    bc_2 = jnp.ones_like(y_b1) * P2\n",
    "    BC_2 = jnp.concatenate([x_b2, y_b2, bc_2], axis=1)\n",
    "\n",
    "    # Bottom BC: P_y[x,0] = 0\n",
    "    x_b3 = jax.random.uniform(bk3, minval=xmin, maxval=xmax, shape=(N_b, 1))\n",
    "    y_b3 = jnp.zeros_like(x_b3)\n",
    "    bc_3 = jnp.ones_like(x_b3) * dP_dy1\n",
    "    BC_3 = jnp.concatenate([x_b3, y_b3, bc_3], axis=1)\n",
    "\n",
    "    # Top BC: P_y[x,1] = 0\n",
    "    x_b4 = jax.random.uniform(bk4, minval=xmin, maxval=xmax, shape=(N_b, 1))\n",
    "    y_b4 = jnp.ones_like(x_b4)\n",
    "    bc_4 = jnp.ones_like(x_b4) * dP_dy1\n",
    "    BC_4 = jnp.concatenate([x_b4, y_b4, bc_4], axis=1)\n",
    "\n",
    "    conds = [BC_1, BC_2, BC_3, BC_4]\n",
    "\n",
    "    # Collocation points\n",
    "    y_c = jax.random.uniform(r1k, minval=ymin, maxval=ymax, shape=(N_r, 1))\n",
    "    x_c = jax.random.uniform(r2k, minval=xmin, maxval=xmax, shape=(N_r, 1))\n",
    "    colloc = jnp.concatenate([x_c, y_c], axis=1)\n",
    "\n",
    "    return x_b1, y_b1, bc_1, x_b2, y_b2, bc_2, x_b3, y_b3, bc_3, x_b4, y_b4, bc_4, x_c, y_c, conds, colloc\n",
    "\n",
    "\n",
    "# call the function to create boundary conditions and collocation points\n",
    "x_b1, y_b1, bc_1, x_b2, y_b2, bc_2, x_b3, y_b3, bc_3, x_b4, y_b4, bc_4, x_c, y_c, conds, colloc \\\n",
    "= generate_BCs_and_colloc(N_b, N_r, ymin, ymax, xmin, xmax, P1, P2, dP_dy1, dP_dy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819b6bb9-a56f-41fc-ac6a-f6885bd4938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary_conditions(x_b1, y_b1, bc_1, x_b2, y_b2, bc_2, x_b3, y_b3, bc_3, x_b4, y_b4, bc_4, x_c, y_c):\n",
    "    plt.figure(figsize=(5, 4), dpi=150)\n",
    "    \n",
    "    # Calculate vmin and vmax based on all boundary condition values\n",
    "    all_bc_values = np.concatenate([bc_1, bc_2, bc_3, bc_4])\n",
    "    vmin, vmax = all_bc_values.min(), all_bc_values.max()\n",
    "    \n",
    "    # Plot each set of boundary conditions with dynamic vmin and vmax\n",
    "    s = 10\n",
    "    scatter1 = plt.scatter(x_b1, y_b1, c=bc_1, marker='x', vmin=vmin, vmax=vmax, label='P[0,y]', cmap=cm.jet, s=s)\n",
    "    plt.scatter(x_b2, y_b2, c=bc_2, marker='^', vmin=vmin, vmax=vmax, label='P[1,y]', cmap=cm.jet, s=s)\n",
    "    plt.scatter(x_b3, y_b3, c=bc_3, marker='*', vmin=vmin, vmax=vmax, label='$\\\\partial P/\\\\partial y[x,0]$', cmap=cm.jet, s=s)\n",
    "    plt.scatter(x_b4, y_b4, c=bc_4, marker='o', vmin=vmin, vmax=vmax, label='$\\\\partial P/\\\\partial y[x,1]$', cmap=cm.jet, s=s)\n",
    "    \n",
    "    # Plot collocation points\n",
    "    s = 5\n",
    "    plt.scatter(x_c, y_c, c='k', marker='.', alpha=0.5, label='Collocation points', s=s)\n",
    "    \n",
    "    # Labels and colorbar\n",
    "    plt.xlabel('$X$')\n",
    "    plt.ylabel('$Y$')\n",
    "    cbar = plt.colorbar(scatter1, aspect=30)  # Attach colorbar to the first scatter plot\n",
    "    cbar.set_label('$Pressure (kPa)$')\n",
    "    \n",
    "    # Place the legend outside the plot\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2, frameon=False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Call the updated function\n",
    "#plot_boundary_conditions(x_b1, y_b1, bc_1, x_b2, y_b2, bc_2, x_b3, y_b3, bc_3, x_b4, y_b4, bc_4, x_c, y_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1faa22-9bbd-44f2-b17c-a6b6f8f7ce50",
   "metadata": {},
   "source": [
    "## 3. Define PDE Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f320b18-b068-413e-a369-eca7f8cf64ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ∂^2P/∂x^2 + ∂^2P/∂y^2 = 0 \n",
    "def pde_residual_2d_darcy(x,y,P):\n",
    "    \"\"\"\n",
    "    Compute the PDE residual for the 2D Laplace equation.\n",
    "    \n",
    "    Inputs:\n",
    "    x, y: Coordinates of collocation points (N x 1).\n",
    "    P: Lambda function representing the neural network solution P(x, y).\n",
    "    \n",
    "    Output:\n",
    "    Residual of the PDE at the collocation points (N x 1).\n",
    "    \"\"\"\n",
    "    P_x = lambda x,y:jax.grad(lambda x,y:jnp.sum(P(x,y)),0)(x,y)\n",
    "    P_xx = lambda x,y:jax.grad(lambda x,y:jnp.sum(P_x(x,y)),0)(x,y)\n",
    "    \n",
    "    P_y=lambda x,y:jax.grad(lambda x,y:jnp.sum(P(x,y)),1)(x,y)\n",
    "    P_yy=lambda x,y:jax.grad(lambda x,y:jnp.sum(P_y(x,y)),1)(x,y)\n",
    "    \n",
    "    return P_xx(x,y) + P_yy(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234afdce-efe7-421e-9e4c-85aed67973cc",
   "metadata": {},
   "source": [
    "## 4. Define Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f713ab-d991-4d16-b252-b545943db5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network Architecture \n",
    "def neural_net(params, x, y):\n",
    "    \"\"\"\n",
    "    Forward pass of a neural network to predict P(x, y).\n",
    "    \n",
    "    Inputs:\n",
    "    params: Neural network parameters (list of weights and biases).\n",
    "    x, y: Input coordinates (N x 1).\n",
    "    \n",
    "    Output:\n",
    "    Predicted P(x, y) values (N x 1).\n",
    "    \"\"\"\n",
    "    X = jnp.concatenate([x, y],axis=1)\n",
    "    *hidden,last = params\n",
    "    for layer in hidden :\n",
    "        X = jax.nn.tanh(X@layer['W']+layer['B'])\n",
    "    return X@last['W'] + last['B']\n",
    "\n",
    "\n",
    "# Function to initialize parameters of the neural network (weights and biases)\n",
    "def init_params(layers):\n",
    "    \"\"\"\n",
    "    Initialize parameters (weights and biases) for a neural network with specified layers.\n",
    "    \n",
    "    Args:\n",
    "        layers: List of integers representing the number of nodes in each layer.\n",
    "                For example, [2, 20, 20, 1] creates a network with input layer (2 nodes),\n",
    "                two hidden layers (20 nodes each), and an output layer (1 node).\n",
    "    \n",
    "    Returns:\n",
    "        params: List of dictionaries containing 'W' (weights) and 'B' (biases) for each layer.\n",
    "    \"\"\"\n",
    "    keys = jax.random.split(jax.random.PRNGKey(0), len(layers) - 1)  # Generate random keys for each layer\n",
    "    params = list()  # Initialize a list to store layer parameters\n",
    "    \n",
    "    for key, n_in, n_out in zip(keys, layers[:-1], layers[1:]):  # Loop through layer dimensions\n",
    "        lb, ub = -(1 / jnp.sqrt(n_in)), (1 / jnp.sqrt(n_in))  # Xavier initialization bounds\n",
    "        W = lb + (ub - lb) * jax.random.uniform(key, shape=(n_in, n_out))  # Initialize weights\n",
    "        B = jax.random.uniform(key, shape=(n_out,))  # Initialize biases\n",
    "        params.append({'W': W, 'B': B})  # Append layer parameters (weights and biases) to the list\n",
    "    \n",
    "    return params  # Return the initialized parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53da33c2-44a9-4f0f-954e-e8646e5dd3ab",
   "metadata": {},
   "source": [
    "## 5. Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc70b98-d332-4815-9529-0e509a579d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fun(params, colloc, conds):\n",
    "    \"\"\"\n",
    "    Calculate the total loss for the PINN.\n",
    "    \n",
    "    Inputs:\n",
    "        params: Neural network parameters (list of weights and biases).\n",
    "        colloc: Collocation points for the PDE residual (N x 2).\n",
    "        conds: Boundary condition data (list of 4 elements, one for each BC).\n",
    "    \n",
    "    Output:\n",
    "        Scalar total loss value.\n",
    "    \"\"\"\n",
    "    x_c, y_c = colloc[:, [0]], colloc[:, [1]]\n",
    "    \n",
    "    P_nn = lambda x, y: neural_net(params, x, y)\n",
    "    \n",
    "    pde_loss = jnp.mean(pde_residual_2d_darcy(x_c, y_c, P_nn)**2)\n",
    "    \n",
    "    # Loss at the left and right Dirichlet BCs \n",
    "    dbc_loss = 0\n",
    "    for cond in conds[0:2]:\n",
    "        x_b, y_b, u_b = cond[:, [0]], cond[:, [1]], cond[:, [2]]  \n",
    "        #dbc_loss += MSE(P_nn(x_b, y_b), u_b)  # Dirichlet BC loss\n",
    "        dbc_loss += jnp.mean(P_nn(x_b, y_b) - u_b)**2  # Dirichlet BC loss\n",
    "    \n",
    "    # Loss at the bottom and top Neumann BCs \n",
    "    nbc_loss = 0\n",
    "    P_nn_y = lambda x, y: jax.grad(lambda x, y: jnp.sum(P_nn(x, y)), 1)(x, y)  # Neumann BCs derivative\n",
    "    for cond in conds[2:4]:\n",
    "        x_b, y_b, u_b = cond[:, [0]], cond[:, [1]], cond[:, [2]]  \n",
    "        #nbc_loss += MSE(P_nn_y(x_b, y_b), u_b)  # Neumann BC loss\n",
    "        nbc_loss += jnp.mean(P_nn_y(x_b, y_b) - u_b)**2  # Neumann BC loss\n",
    "\n",
    "    # Total loss (summed components)\n",
    "    loss = pde_loss + dbc_loss + nbc_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a3d13-ac48-492e-9abe-9045f5da8fb3",
   "metadata": {},
   "source": [
    "## 6. Define Hyperparameters and Training Loop\n",
    "This cell contains the training loop for the Physics-Informed Neural Network (PINN). It includes functions and code to update the network parameters, compute the loss, track training progress, and identify the best-performing model during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d526f73-b808-4daa-a417-2d0ebd5cdc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN training started...\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for the neural network and training\n",
    "hidden_nodes = 20      # Number of nodes in each hidden layer\n",
    "hidden_layers = 3      # Number of hidden layers in the neural network\n",
    "lr = 1e-3              # Learning rate for the optimizer\n",
    "epochs = 2000          # Number of epochs for training\n",
    "\n",
    "\n",
    "# Construct the MLP (Multilayer Perceptron) using the specified architecture\n",
    "params = init_params([2] + [hidden_nodes] * hidden_layers + [1])  \n",
    "# The network has 2 input nodes, `hidden_layers` layers with `hidden_nodes`, and 1 output node.\n",
    "\n",
    "# Define the optimizer (Adam optimizer with the specified learning rate)\n",
    "optimizer = optax.adam(lr)\n",
    "\n",
    "# Initialize the optimizer state using the network parameters\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parameters update function\n",
    "@jax.jit\n",
    "def update(opt_state,params,colloc,conds):\n",
    "    \"\"\"\n",
    "    Perform a single optimization step:\n",
    "    - Calculate gradients of the loss with respect to network parameters.\n",
    "    - Update the parameters using the optimizer.\n",
    "    \n",
    "    Inputs:\n",
    "    opt_state: Current optimizer state.\n",
    "    params: Current neural network parameters (weights and biases).\n",
    "    colloc: Collocation points for the PDE residual loss.\n",
    "    conds: Boundary condition data for Dirichlet and Neumann conditions.\n",
    "    \n",
    "    Outputs:\n",
    "    opt_state: Updated optimizer state.\n",
    "    params: Updated network parameters.\n",
    "    \"\"\"\n",
    "    # Get the gradient w.r.t to MLP params\n",
    "    grads=jax.jit(jax.grad(loss_fun,0))(params,colloc,conds)\n",
    "    \n",
    "    #Update params\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return opt_state, params\n",
    "\n",
    "\n",
    "#%% PINN training loop\n",
    "print('PINN training started...')\n",
    "\n",
    "# Initialize tracking variables\n",
    "best_params = params\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "# Initialize lists to track all losses and epochs\n",
    "all_losses = []\n",
    "all_epochs = []\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(epochs+1):\n",
    "#     # Update the optimizer state and parameters\n",
    "#     opt_state, params = update(opt_state, params, colloc, conds)\n",
    "\n",
    "#     # Calculate the current loss\n",
    "#     current_loss = loss_fun(params, colloc, conds)\n",
    "    \n",
    "#     # Store loss and epoch\n",
    "#     all_losses.append(current_loss)\n",
    "#     all_epochs.append(epoch)\n",
    "    \n",
    "#     # Update the best parameters if the current loss is the lowest\n",
    "#     if current_loss < best_loss:\n",
    "#         best_loss = current_loss\n",
    "#         best_params = params\n",
    "#         best_epoch = epoch\n",
    "\n",
    "#     # Print loss and epoch info\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f'Epoch={epoch}\\tloss={current_loss:.3e}')\n",
    "\n",
    "# print('PINN training done!')\n",
    "# print(f'Best Epoch = {best_epoch}\\tBest Loss = {best_loss:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23685df-c2f3-41ad-ad09-079ed779067c",
   "metadata": {},
   "source": [
    "## 7. Plot pinn loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b1d75d2-16f9-457d-9f7e-90d75c855851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for plotting for PINN losses during training \n",
    "def plot_pinn_training(all_losses, all_epochs):\n",
    "    plt.figure(figsize = (5, 4), dpi = 150)\n",
    "    plt.semilogy(all_epochs, all_losses, '-r', markersize = 4, linewidth = 1.0)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #plt.legend(prop={\"size\": fontsize_legend}, loc=\"best\")\n",
    "    plt.xlim(min(all_epochs), max(all_epochs))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "#plot_pinn_training(all_losses, all_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939a8ff-3b9a-428f-88b4-45b554b6ea50",
   "metadata": {},
   "source": [
    "## 8. PINN prediction and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb748ab3-ac73-4947-9d74-5a5b78b258cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% PINN prediction and plotting results\n",
    "nx = 20\n",
    "x = np.linspace(xmin, xmax, nx)\n",
    "dx = x[1] - x[0]\n",
    "ny = 20\n",
    "y = np.linspace(ymin, ymax, ny)\n",
    "dy = y[1] - y[0]\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# pinn_P = (neural_net(best_params, X.flatten().reshape(-1,1), Y.flatten().reshape(-1,1))).reshape(nx,ny) \n",
    "# print(pinn_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbaf3068-c6c4-4f0f-9b07-679348bbf566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_pressure_distribution(X, Y, pressure, title, cbar_lebel):\n",
    "    plt.figure(figsize = (5, 4), dpi = 150)\n",
    "    cmap = 'jet'\n",
    "    contour = plt.contourf(X, Y, pressure, levels=100, cmap=cmap)  \n",
    "    plt.xlabel('$X$')\n",
    "    plt.ylabel('$Y$')\n",
    "    # Colorbar settings\n",
    "    vmin = np.min(pressure)\n",
    "    vmax = np.max(pressure)\n",
    "    #cbar = plt.colorbar(mpl.cm.ScalarMappable(norm=mpl.colors.Normalize(vmin = vmin, vmax = vmax), cmap=cm.jet), aspect=30)\n",
    "    cbar = plt.colorbar(contour, aspect=30)\n",
    "\n",
    "\n",
    "    cbar.set_label(f'{cbar_lebel}')\n",
    "    plt.xlim(np.min(X), np.max(X))\n",
    "    plt.ylim(np.min(Y), np.max(Y))\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "title = \"PINN prediction\"\n",
    "cbar_lebel = \"$Pressure (kPa)$\"\n",
    "#plot_2d_pressure_distribution(X, Y, pinn_P, title, cbar_lebel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e374911b-c1d6-4cf6-9e58-705ded4dd3ba",
   "metadata": {},
   "source": [
    "## 9. Solution verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b90b1953-0132-44d4-ab84-312b61d6b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Analytical solution\n",
    "L = xmax\n",
    "analytical_P = P1 + (P2 - P1) * X / L\n",
    "#print(analytical_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceec87c9-d5b5-4989-8776-a9edbe2d73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Analytical solution\"\n",
    "cbar_lebel = \"$Pressure (kPa)$\"\n",
    "#plot_2d_pressure_distribution(X, Y, analytical_P, title, cbar_lebel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c5891-acc5-4d3b-8c2b-f1104cb3cafd",
   "metadata": {},
   "source": [
    "## 10. Error analysis between the analytical solution and PINN prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7ed8106-6f33-46db-9978-6060c8118e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Flatten the 2D arrays into 1D\n",
    "# analytical_Pr = analytical_P.flatten().reshape(-1, 1)  # Flatten to 1D and reshape to 2D (n_samples, n_features)\n",
    "# pinn_Pr = pinn_P.flatten()                       # Flatten to 1D (n_samples,)\n",
    "\n",
    "# # Perform regression\n",
    "# reg = LinearRegression()\n",
    "# reg.fit(analytical_Pr, pinn_Pr)  # Fit the model\n",
    "\n",
    "# # Predict using the regression model\n",
    "# y_pred = reg.predict(analytical_Pr)\n",
    "\n",
    "# # Calculate R^2 score\n",
    "# r2 = r2_score(pinn_Pr, y_pred)\n",
    "\n",
    "# # Calculate RMSE\n",
    "# mse = mean_squared_error(pinn_Pr, y_pred)  # Mean Squared Error\n",
    "# rmse = np.sqrt(mse)  # Root Mean Squared Erro\n",
    "\n",
    "# # Print results\n",
    "# print(f\"R^2 Score: {r2:.4f}\")\n",
    "# print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "# #%% Plotting Regression\n",
    "# plt.figure(figsize=(5, 4), dpi = 150)\n",
    "# plt.scatter(analytical_Pr, pinn_Pr, label=\"Data Points\", color=\"blue\", alpha=0.5, s=40)\n",
    "# plt.plot(analytical_Pr, y_pred, label=\"Regression Line\", color=\"red\", linewidth=2)\n",
    "\n",
    "# # Add annotations for RMSE and R^2\n",
    "# plt.text(\n",
    "#     0.05,\n",
    "#     0.95,\n",
    "#     f\"$R^2$ = {r2:.4f}\\nRMSE = {rmse:.4f}\",\n",
    "#     transform=plt.gca().transAxes,\n",
    "#     fontsize=12,\n",
    "#     verticalalignment=\"top\",\n",
    "#     bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8),\n",
    "# )\n",
    "\n",
    "# plt.xlabel(\"Analytical Solution (Flattened)\")\n",
    "# plt.ylabel(\"PINN Prediction (Flattened)\")\n",
    "# plt.title(\"Regression Plot (Flattened 2D)\")\n",
    "# plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ca4b78f-4510-4a93-8beb-a630df9ded3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the percentage error\n",
    "# error_percentage = 100 * (analytical_P - pinn_P) / (analytical_P)\n",
    "# title = \"Error between analytical solution and PINN prediction\"\n",
    "# cbar_lebel = \"$Error (\\%)$\"\n",
    "#plot_2d_pressure_distribution(X, Y, error_percentage, title, cbar_lebel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a5114-d0c4-4ff6-b0b5-6c49b6b3d912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
